<!-- TOC -->

- [报警对象分为两种](#报警对象分为两种)
- [采集什么指标](#采集什么指标)
    - [对于数据库来说，我们可以采集：](#对于数据库来说我们可以采集)
- [也可以分为四层，上一层对下一层的资源消耗量变成了下一层的请求数](#也可以分为四层上一层对下一层的资源消耗量变成了下一层的请求数)
- [三种场合需要算法](#三种场合需要算法)
- [异常检测](#异常检测)
- [总结](#总结)

<!-- /TOC -->
# 报警对象分为两种
* 业务规则监控
    * 业务规则监控的不是硬件，也不是软件是否工作正常。而是软件是否按照业务规则实现的，是否有漏洞。也可以理解为对“正确性”的监控
    * 又比如斗地主游戏里一个人的连胜场次是有一定上限的，每天的胜率是有一定上限，如果超出平均值太多就可能是作弊
    * 比如DNF的游戏角色在一定装备的情况下，单次打击的伤害输出应该是有一个上限，如果超过了就说明有作弊的情况

* 系统可靠性监控
	* 系统可靠性监控是最常见的监控形式，比如发现是不是服务器挂掉了，服务是不是过载了等等    

# 采集什么指标
* 请求数，请求到达速率
* 正常响应数，正常响应占比
* 错误响应数，错误响应占比
* 响应延时
* 队列长度，排队时间
* CPU使用率
* 内存分配和释放
* 网络发送包量
## 对于数据库来说，我们可以采集：
* cpu 使用率
* 网络带宽大小
* db请求数
* db响应数
* db错误响应数
* db请求延迟
* db请求数的绝对量
* db正确响应相对请求数的占比
# 也可以分为四层，上一层对下一层的资源消耗量变成了下一层的请求数
* 产品策略和营销：它们决定了根本的请求到达的速率
* 应用层（更粗俗一点可以叫web层）：最上层的胶水
* 服务层：db，各种RPC服务，以及层层嵌套的服务
* 硬件层：cpu，内存，磁盘，网络
# 三种场合需要算法
* 无法直接采集到错误数：需要对错误日志的自动分类
* 无法直接采集到请求成功率：需要对请求数或响应数的绝对值做异常检测
* 只有总数，无法采集到其中的每个细分构成项的占比：需要对参与的factor进行算法拟合
# 异常检测
* 曲线平滑：故障一般是对近期趋势的一个破坏，视觉上来说就是不平滑
* 绝对值的时间周期性：两条曲线几乎重合
* 波动的时间周期性：假设两个曲线不重合，在相同时间点的波动趋势和振幅也是类似的
* 有一个长度可观的坑：当曲线开始回升到历史范围的时候，一般可以确认这个时间段是真的故障了
# 总结
* 不应该用采集的难度决定你使用什么指标去告警
* 请求数 + 成功率
* 响应延迟
* 只要采集对了指标，大部分时候告警不需要复杂算法
* 基于算法的异常检测：算法不难，实在必要也是可以做到的